# Preference-based RL on Taxi-v3 Project

## Structure du projet

```
taxi-pbrl-project/
â”œâ”€â”€ notebooks/          # Notebooks Jupyter pour expÃ©rimentation
â”‚   â””â”€â”€ test_taxi_env.py
â”œâ”€â”€ src/               # Code source principal
â”‚   â””â”€â”€ q_learning_agent.py
â”œâ”€â”€ results/           # RÃ©sultats et modÃ¨les sauvegardÃ©s
â”œâ”€â”€ docs/             # Documentation
â””â”€â”€ train_classical_agent.py  # Script d'entraÃ®nement
```

## Installation

1. Cloner/crÃ©er le projet
2. Installer les dÃ©pendances:
   ```bash
   pip install gymnasium numpy matplotlib pygame
   pip install "gymnasium[toy-text]"
   ```

## Phase 1: Agent Q-Learning Classique âœ…

### Utilisation

**1. EntraÃ®nement de l'agent classique:**
```bash
python train_classical_agent.py
```

**2. DÃ©monstration du systÃ¨me de prÃ©fÃ©rences (ğŸ†• avec visualisation graphique):**
```bash
python demo_preferences.py
```
> ğŸ¬ **Nouveau !** Les trajectoires s'affichent maintenant visuellement cÃ´te Ã  cÃ´te avec le rendu Gymnasium !

**3. Test rapide de la visualisation:**
```bash
python test_visual_preference.py
```

**4. Rejouer des trajectoires sauvegardÃ©es:**
```bash
python test_visual_replay.py
```

**5. EntraÃ®nement et comparaison avec l'agent PbRL:**
```bash
python train_pbrl_agent.py
```

**6. Analyse statistique avancÃ©e:**
```bash
python statistical_analysis.py
```

### ğŸ¬ Nouvelle FonctionnalitÃ©: Visualisation Graphique

Le systÃ¨me de prÃ©fÃ©rences inclut maintenant une **visualisation Gymnasium interactive** :
- ğŸ“º Affichage cÃ´te Ã  cÃ´te des deux trajectoires
- ğŸ® ContrÃ´les interactifs (pause, replay)
- ğŸ“Š Statistiques en temps rÃ©el
- ğŸ¯ Interface intuitive pour comparer visuellement

**Voir le guide complet:** [`docs/visual_preferences_guide.md`](docs/visual_preferences_guide.md)

### Modes d'entraÃ®nement PbRL:
1. **Mode automatique**: Utilise des prÃ©fÃ©rences simulÃ©es
2. **Mode interactif** ğŸ†•: Collecte tes prÃ©fÃ©rences en temps rÃ©el avec visualisation graphique
3. **Mode standard**: Agent normal pour comparaison

## Fichiers gÃ©nÃ©rÃ©s

### ğŸ“ **RÃ©sultats (`results/`):**
- `q_learning_agent_classical.pkl` - Agent classique entraÃ®nÃ©
- `pbrl_agent.pkl` - Agent PbRL entraÃ®nÃ©  
- `demo_trajectories.pkl` - Trajectoires de dÃ©monstration
- `comparison_classical_vs_pbrl.png` - Graphiques de comparaison
- `advanced_statistical_analysis.png` - Analyse statistique complÃ¨te
- `detailed_comparison.json` - DonnÃ©es dÃ©taillÃ©es des rÃ©sultats
- `performance_report.md` - Rapport de performance statistique

### ğŸ“ **Documentation (`docs/`):**
- `detailed_analysis.md` - Analyse approfondie des rÃ©sultats
- `final_insights.md` - Insights finaux et conclusions

### Fichiers disponibles

**Agents:**
- `src/q_learning_agent.py`: Agent Q-Learning classique
- `src/pbrl_agent.py`: Agent Preference-based RL
- `src/trajectory_manager.py`: Gestion et comparaison des trajectoires
- `src/preference_interface.py`: Interface de collecte de prÃ©fÃ©rences

**Scripts principaux:**
- `train_classical_agent.py`: EntraÃ®nement agent classique
- `train_pbrl_agent.py`: EntraÃ®nement agent PbRL et comparaison
- `demo_preferences.py`: DÃ©monstration du systÃ¨me de prÃ©fÃ©rences

**Tests:**
- `notebooks/test_taxi_env.py`: Test de base de l'environnement

## Phases terminÃ©es âœ…

### Phase 1: Agent Q-Learning Classique âœ…
- âœ… Agent Q-Learning fonctionnel
- âœ… EntraÃ®nement et Ã©valuation
- âœ… Sauvegarde et mÃ©triques

### Phase 2: SystÃ¨me de PrÃ©fÃ©rences âœ…
- âœ… Module de comparaison de trajectoires
- âœ… Interface de saisie des prÃ©fÃ©rences
- âœ… Visualisation et analyse automatique

### Phase 3: Agent PbRL (Preference-based RL) âœ…
- âœ… Agent PbRL avec apprentissage par prÃ©fÃ©rences
- âœ… Boucle d'apprentissage interactive
- âœ… Conversion prÃ©fÃ©rences â†’ signal d'apprentissage

## Phase 4: ExpÃ©rimentations âœ…

### RÃ©sultats Finaux ğŸ¯
- âœ… **Comparaison complÃ¨te** classique vs PbRL rÃ©alisÃ©e
- âœ… **Analyse statistique rigoureuse** avec tests de significativitÃ©  
- âœ… **MÃ©triques dÃ©taillÃ©es** et visualisations avancÃ©es
- âœ… **Insights approfondis** documentÃ©s

### ğŸ“Š **RÃ©sultats ClÃ©s**
- **PbRL**: 8.11 Â± 2.40 points (6k Ã©pisodes d'entraÃ®nement)
- **Classique**: 7.95 Â± 2.68 points (15k Ã©pisodes d'entraÃ®nement)
- **AmÃ©lioration**: +2.01% avec **60% moins d'Ã©pisodes**
- **Variance rÃ©duite**: -11% (comportement plus stable)

### ğŸ”¬ **SignificativitÃ© Statistique**
- **Tests**: t-test, Mann-Whitney U, Kolmogorov-Smirnov
- **Cohen's d**: 0.062 (effet nÃ©gligeable)
- **Conclusion**: AmÃ©lioration non statistiquement significative mais efficacitÃ© d'entraÃ®nement prouvÃ©e

## Prochaines Ã©tapes

### Phase 5: Finalisation ğŸ“
- [ ] RÃ©daction du rapport final (3-4 pages)
- [ ] PrÃ©paration de la prÃ©sentation
- [ ] Documentation des extensions possibles

## Environnement Taxi-v3

- **Ã‰tats**: 500 (position taxi, passager, destination)
- **Actions**: 6 (Nord, Sud, Est, Ouest, Prendre, DÃ©poser)
- **Objectif**: Prendre le passager et le dÃ©poser Ã  destination
- **RÃ©compenses**: 
  - +20: livraison rÃ©ussie
  - -10: action illÃ©gale (prendre/dÃ©poser)
  - -1: chaque pas de temps