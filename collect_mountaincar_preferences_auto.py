"""
Collection automatique de pr√©f√©rences pour MountainCar (mode simulation)
Pour tester rapidement le workflow PBRL sans interaction manuelle
"""

import gymnasium as gym
import numpy as np
import pickle
import json
import os
from datetime import datetime
from src.mountain_car_agent import MountainCarAgent
from collect_mountaincar_preferences import collect_mountaincar_trajectory, MountainCarTrajectory


def auto_select_preference(traj_a: MountainCarTrajectory, traj_b: MountainCarTrajectory) -> int:
    """
    S√©lection automatique bas√©e sur des crit√®res objectifs
    
    Returns:
        1 si A est meilleure, 2 si B est meilleure, 0 si √©galit√©
    """
    # Crit√®re 1: Succ√®s vs √©chec
    if traj_a.success and not traj_b.success:
        return 1
    if traj_b.success and not traj_a.success:
        return 2
    
    # Crit√®re 2: Si les deux r√©ussissent, pr√©f√©rer la plus rapide
    if traj_a.success and traj_b.success:
        if abs(traj_a.episode_length - traj_b.episode_length) > 2:
            return 1 if traj_a.episode_length < traj_b.episode_length else 2
    
    # Crit√®re 3: Si les deux √©chouent, pr√©f√©rer celle qui monte plus haut
    if not traj_a.success and not traj_b.success:
        position_diff = abs(traj_a.max_position - traj_b.max_position)
        if position_diff > 0.05:
            return 1 if traj_a.max_position > traj_b.max_position else 2
    
    # Crit√®re 4: Diff√©rence de r√©compense significative
    reward_diff = abs(traj_a.total_reward - traj_b.total_reward)
    if reward_diff > 5:
        return 1 if traj_a.total_reward > traj_b.total_reward else 2
    
    # Sinon, √©galit√©
    return 0


def main():
    """Collection automatique de pr√©f√©rences"""
    
    print(f"\n{'='*80}")
    print("ü§ñ COLLECTE AUTOMATIQUE DE PR√âF√âRENCES - MOUNTAINCAR-V0")
    print(f"{'='*80}\n")
    
    # Configuration
    N_TRAJECTORIES = 50
    N_PREFERENCES = 25
    
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)
    
    # Chargement de l'agent
    agent_path = os.path.join(results_dir, "mountain_car_agent_classical.pkl")
    
    if not os.path.exists(agent_path):
        print(f"‚ùå Agent non trouv√©: {agent_path}")
        print("üí° Ex√©cutez d'abord: python train_mountaincar_classical.py")
        return
    
    print(f"üìÇ Chargement de l'agent...")
    agent = MountainCarAgent()
    agent.load_agent(agent_path)
    print("‚úÖ Agent charg√©\n")
    
    # Environnement
    print("üåç Cr√©ation de l'environnement...")
    env = gym.make('MountainCar-v0')
    print("‚úÖ Environnement cr√©√©\n")
    
    # G√©n√©ration de trajectoires
    print(f"üé¨ G√âN√âRATION DE {N_TRAJECTORIES} TRAJECTOIRES")
    print("-" * 80)
    
    trajectories = []
    for i in range(N_TRAJECTORIES):
        traj = collect_mountaincar_trajectory(env, agent, episode_id=i)
        trajectories.append(traj)
        
        if (i + 1) % 10 == 0:
            successes = sum(1 for t in trajectories if t.success)
            print(f"  Trajectoire {i + 1}/{N_TRAJECTORIES} | "
                  f"Succ√®s: {successes}/{i + 1} ({successes/(i+1)*100:.1f}%)")
    
    total_successes = sum(1 for t in trajectories if t.success)
    print(f"\n‚úÖ {N_TRAJECTORIES} trajectoires g√©n√©r√©es")
    print(f"   Taux de succ√®s: {total_successes}/{N_TRAJECTORIES} ({total_successes/N_TRAJECTORIES*100:.1f}%)\n")
    
    # S√©lection de paires
    print(f"üìä S√âLECTION DE {N_PREFERENCES} PAIRES")
    print("-" * 80)
    
    pairs = []
    
    # Tri par performance
    sorted_trajs = sorted(trajectories, key=lambda t: (t.success, t.total_reward), reverse=True)
    
    # Cr√©er des paires vari√©es
    for i in range(0, min(N_PREFERENCES * 2, len(sorted_trajs) - 1), 2):
        if i + 1 < len(sorted_trajs):
            pairs.append((sorted_trajs[i], sorted_trajs[i + 1]))
    
    # Compl√©ter avec paires al√©atoires si n√©cessaire
    while len(pairs) < N_PREFERENCES and len(trajectories) >= 2:
        idx_a = np.random.randint(0, len(trajectories))
        idx_b = np.random.randint(0, len(trajectories))
        if idx_a != idx_b:
            pairs.append((trajectories[idx_a], trajectories[idx_b]))
    
    pairs = pairs[:N_PREFERENCES]
    print(f"‚úÖ {len(pairs)} paires s√©lectionn√©es\n")
    
    # Collection automatique
    print(f"ü§ñ COLLECTE AUTOMATIQUE DE {len(pairs)} PR√âF√âRENCES")
    print("-" * 80)
    
    preferences = []
    
    for idx, (traj_a, traj_b) in enumerate(pairs):
        # S√©lection automatique
        choice = auto_select_preference(traj_a, traj_b)
        
        # Affichage
        if (idx + 1) % 5 == 0:
            print(f"  Pr√©f√©rence {idx + 1}/{len(pairs)} collect√©e")
        
        # Enregistrement
        preference_data = {
            'trajectory_a_id': int(traj_a.episode_id),
            'trajectory_b_id': int(traj_b.episode_id),
            'choice': int(choice),
            'trajectory_a_reward': float(traj_a.total_reward),
            'trajectory_b_reward': float(traj_b.total_reward),
            'trajectory_a_length': int(traj_a.episode_length),
            'trajectory_b_length': int(traj_b.episode_length),
            'trajectory_a_success': bool(traj_a.success),
            'trajectory_b_success': bool(traj_b.success),
            'trajectory_a_max_position': float(traj_a.max_position),
            'trajectory_b_max_position': float(traj_b.max_position),
            'trajectory_a_efficiency': float(traj_a.total_reward / traj_a.episode_length),
            'trajectory_b_efficiency': float(traj_b.total_reward / traj_b.episode_length),
            'timestamp': datetime.now().isoformat(),
            'auto_selected': True
        }
        preferences.append(preference_data)
    
    env.close()
    
    # Sauvegarde
    print(f"\nüíæ SAUVEGARDE DES DONN√âES")
    print("-" * 80)
    
    # Pr√©f√©rences
    preferences_path = os.path.join(results_dir, "mountaincar_preferences.json")
    with open(preferences_path, 'w') as f:
        json.dump(preferences, f, indent=2)
    print(f"‚úÖ Pr√©f√©rences sauvegard√©es: {preferences_path}")
    
    # Trajectoires
    trajectories_path = os.path.join(results_dir, "mountaincar_trajectories.pkl")
    with open(trajectories_path, 'wb') as f:
        pickle.dump(trajectories, f)
    print(f"‚úÖ Trajectoires sauvegard√©es: {trajectories_path}")
    
    # Statistiques
    print(f"\n{'='*80}")
    print("üìä STATISTIQUES DES PR√âF√âRENCES")
    print(f"{'='*80}")
    print(f"Total pr√©f√©rences: {len(preferences)}")
    
    choices_a = sum(1 for p in preferences if p['choice'] == 1)
    choices_b = sum(1 for p in preferences if p['choice'] == 2)
    choices_equal = sum(1 for p in preferences if p['choice'] == 0)
    
    print(f"  Trajectoire A pr√©f√©r√©e: {choices_a} ({choices_a/len(preferences)*100:.1f}%)")
    print(f"  Trajectoire B pr√©f√©r√©e: {choices_b} ({choices_b/len(preferences)*100:.1f}%)")
    print(f"  √âgalit√©: {choices_equal} ({choices_equal/len(preferences)*100:.1f}%)")
    print(f"{'='*80}\n")
    
    print("‚úÖ Collection automatique termin√©e!")
    print("   Prochaine √©tape: python train_mountaincar_pbrl.py\n")


if __name__ == "__main__":
    main()
