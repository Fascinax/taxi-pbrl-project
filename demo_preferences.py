import gymnasium as gym
import numpy as np
from src.q_learning_agent import QLearningAgent
from src.trajectory_manager import TrajectoryManager, Trajectory
from src.preference_interface import PreferenceInterface
import pickle
import os

def main():
    """
    Script pour dÃ©montrer le systÃ¨me de comparaison de trajectoires et de prÃ©fÃ©rences
    """
    
    print("=== DÃ‰MONSTRATION DU SYSTÃˆME DE PRÃ‰FÃ‰RENCES ===")
    
    # Chargement de l'agent prÃ©-entraÃ®nÃ©
    print("\n1ï¸âƒ£ Chargement de l'agent Q-Learning entraÃ®nÃ©...")
    agent = QLearningAgent(n_states=500, n_actions=6)
    
    try:
        agent.load_agent("results/q_learning_agent_classical.pkl")
        print("âœ… Agent chargÃ© avec succÃ¨s!")
    except FileNotFoundError:
        print("âŒ Agent non trouvÃ©. Veuillez d'abord entraÃ®ner l'agent avec train_classical_agent.py")
        return
    
    # CrÃ©ation de l'environnement
    env = gym.make("Taxi-v3")
    
    # Initialisation des managers
    trajectory_manager = TrajectoryManager()
    preference_interface = PreferenceInterface()
    
    # Collecte de trajectoires
    print("\n2ï¸âƒ£ Collecte de trajectoires de dÃ©monstration...")
    trajectories = []
    
    print("GÃ©nÃ©ration de 10 trajectoires...")
    for i in range(10):
        traj = trajectory_manager.collect_trajectory(env, agent, max_steps=200, render=False)
        trajectories.append(traj)
        print(f"  Trajectoire {i+1}: RÃ©compense = {traj.total_reward}, Longueur = {traj.episode_length}")
    
    # Sauvegarde des trajectoires
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)
    trajectory_manager.save_trajectories(f"{results_dir}/demo_trajectories.pkl")
    
    # SÃ©lection de paires intÃ©ressantes pour comparaison
    print("\n3ï¸âƒ£ SÃ©lection de paires de trajectoires pour comparaison...")
    
    # Tri des trajectoires par rÃ©compense pour crÃ©er des paires intÃ©ressantes
    trajectories_sorted = sorted(trajectories, key=lambda t: t.total_reward, reverse=True)
    
    # CrÃ©ation de paires contrastÃ©es
    interesting_pairs = []
    
    # Paire 1: Meilleure vs moins bonne
    if len(trajectories_sorted) >= 2:
        interesting_pairs.append((trajectories_sorted[0], trajectories_sorted[-1]))
    
    # Paire 2: Deux trajectoires avec rÃ©compenses similaires mais longueurs diffÃ©rentes
    middle_trajectories = trajectories_sorted[2:6] if len(trajectories_sorted) > 5 else trajectories_sorted[1:3]
    if len(middle_trajectories) >= 2:
        # Trier par longueur
        middle_by_length = sorted(middle_trajectories, key=lambda t: t.episode_length)
        if len(middle_by_length) >= 2:
            interesting_pairs.append((middle_by_length[0], middle_by_length[-1]))
    
    # Paire 3: Deux trajectoires alÃ©atoires du milieu
    if len(trajectories_sorted) >= 4:
        mid_idx = len(trajectories_sorted) // 2
        interesting_pairs.append((trajectories_sorted[mid_idx], trajectories_sorted[mid_idx + 1]))
    
    print(f"âœ… {len(interesting_pairs)} paires sÃ©lectionnÃ©es pour comparaison")
    
    # DÃ©monstration de la comparaison
    print("\n4ï¸âƒ£ DÃ©monstration de la comparaison de trajectoires...")
    
    if interesting_pairs:
        print("\nğŸ” EXEMPLE DE COMPARAISON AUTOMATIQUE:")
        traj1, traj2 = interesting_pairs[0]
        trajectory_manager.display_trajectory_comparison(traj1, traj2)
        
        # Visualisation graphique
        print("\nğŸ“Š GÃ©nÃ©ration de la visualisation graphique...")
        trajectory_manager.visualize_trajectories(
            traj1, traj2, 
            save_path=f"{results_dir}/trajectory_comparison_demo.png"
        )
    
    # Interface de prÃ©fÃ©rences (mode dÃ©monstration)
    print("\n5ï¸âƒ£ DÃ©monstration de l'interface de prÃ©fÃ©rences...")
    print("\nğŸ¤– COLLECTE INTERACTIVE DE PRÃ‰FÃ‰RENCES")
    print("Vous allez maintenant pouvoir comparer des trajectoires et exprimer vos prÃ©fÃ©rences.")
    
    demo_choice = input("\nğŸ‘‰ Voulez-vous tester l'interface de prÃ©fÃ©rences ? (y/n): ").strip().lower()
    
    if demo_choice in ['y', 'yes', 'oui', 'o']:
        # Session de prÃ©fÃ©rences interactive avec visualisation Gymnasium
        print("\nğŸ¯ SESSION DE PRÃ‰FÃ‰RENCES INTERACTIVE")
        collected_preferences = preference_interface.collect_preference_batch(
            interesting_pairs[:2],  # Limiter Ã  2 comparaisons pour la dÃ©mo
            trajectory_manager
        )
        
        # Sauvegarde et statistiques
        preference_interface.save_preferences(f"{results_dir}/demo_preferences.json")
        preference_interface.display_preferences_summary()
        
        print("\nâœ… DÃ©monstration des prÃ©fÃ©rences terminÃ©e!")
        
    else:
        print("â­ï¸ Interface de prÃ©fÃ©rences ignorÃ©e.")
    
    # RÃ©sumÃ© et prochaines Ã©tapes
    print("\n" + "="*80)
    print("ğŸ‰ DÃ‰MONSTRATION TERMINÃ‰E")
    print("="*80)
    print("âœ… SystÃ¨me de trajectoires opÃ©rationnel")
    print("âœ… Interface de prÃ©fÃ©rences fonctionnelle")
    print("âœ… Visualisations et comparaisons disponibles")
    
    print(f"\nğŸ“ Fichiers gÃ©nÃ©rÃ©s dans '{results_dir}/':")
    generated_files = [
        "demo_trajectories.pkl (trajectoires collectÃ©es)",
        "trajectory_comparison_demo.png (visualisation)",
    ]
    
    if demo_choice in ['y', 'yes', 'oui', 'o']:
        generated_files.append("demo_preferences.json (prÃ©fÃ©rences collectÃ©es)")
    
    for file in generated_files:
        print(f"   â€¢ {file}")
    
    print(f"\nğŸš€ PROCHAINES Ã‰TAPES:")
    print("   1. CrÃ©er le systÃ¨me de conversion prÃ©fÃ©rences â†’ apprentissage")
    print("   2. ImplÃ©menter l'agent PbRL (Preference-based RL)")
    print("   3. Comparer agent classique vs agent PbRL")
    print("   4. Analyser les rÃ©sultats et rÃ©diger le rapport")
    
    env.close()

if __name__ == "__main__":
    main()