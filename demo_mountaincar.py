"""
Script de d√©monstration de MountainCar-v0
Teste l'environnement et visualise un agent entra√Æn√©
"""

import gymnasium as gym
import numpy as np
import os
import time
from src.mountain_car_agent import MountainCarAgent


def watch_random_agent(episodes: int = 3):
    """
    Observe un agent al√©atoire sur MountainCar
    
    Args:
        episodes: Nombre d'√©pisodes √† observer
    """
    print(f"\n{'='*80}")
    print("üé≤ AGENT AL√âATOIRE - MOUNTAINCAR-V0")
    print(f"{'='*80}\n")
    
    env = gym.make('MountainCar-v0', render_mode='human')
    
    for episode in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        
        print(f"\n[ACTION] √âpisode {episode + 1}/{episodes}")
        print(f"Position initiale: {state[0]:.3f}, Vitesse initiale: {state[1]:.3f}")
        
        while steps < 200:
            action = env.action_space.sample()  # Action al√©atoire
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        success = state[0] >= 0.5
        print(f"R√©sultat: {'[OK] Succ√®s!' if success else '[ERROR] √âchec'}")
        print(f"Position finale: {state[0]:.3f}")
        print(f"R√©compense totale: {total_reward:.0f}")
        print(f"Nombre de pas: {steps}")
    
    env.close()
    print(f"\n{'='*80}\n")


def watch_trained_agent(agent_path: str, episodes: int = 5):
    """
    Observe un agent entra√Æn√© sur MountainCar
    
    Args:
        agent_path: Chemin vers l'agent sauvegard√©
        episodes: Nombre d'√©pisodes √† observer
    """
    print(f"\n{'='*80}")
    print("[AGENT] AGENT ENTRA√éN√â - MOUNTAINCAR-V0")
    print(f"{'='*80}\n")
    
    if not os.path.exists(agent_path):
        print(f"[ERROR] Agent non trouv√©: {agent_path}")
        print("[INFO] Ex√©cutez d'abord: python train_mountaincar_classical.py")
        return
    
    # Chargement de l'agent
    print(f"[LOAD] Chargement de l'agent: {agent_path}")
    agent = MountainCarAgent()
    agent.load_agent(agent_path)
    print("[OK] Agent charg√©\n")
    
    env = gym.make('MountainCar-v0', render_mode='human')
    
    successes = 0
    total_rewards = []
    total_steps_list = []
    
    for episode in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        
        print(f"\n[ACTION] √âpisode {episode + 1}/{episodes}")
        print(f"Position initiale: {state[0]:.3f}, Vitesse initiale: {state[1]:.3f}")
        
        # Analyse de la strat√©gie initiale
        analysis = agent.get_state_analysis(state)
        print(f"Premi√®re action recommand√©e: {analysis['best_action_name']}")
        
        while steps < 200:
            action = agent.select_action(state, training=False)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            total_reward += reward
            steps += 1
            
            # Ralentir un peu pour mieux voir
            time.sleep(0.01)
            
            if done:
                break
        
        success = state[0] >= 0.5
        if success:
            successes += 1
        
        total_rewards.append(total_reward)
        total_steps_list.append(steps)
        
        print(f"R√©sultat: {'[OK] Succ√®s!' if success else '[ERROR] √âchec'}")
        print(f"Position finale: {state[0]:.3f}")
        print(f"R√©compense totale: {total_reward:.0f}")
        print(f"Nombre de pas: {steps}")
    
    env.close()
    
    # Statistiques finales
    print(f"\n{'='*80}")
    print("[PLOT] STATISTIQUES")
    print(f"{'='*80}")
    print(f"Taux de succ√®s: {(successes/episodes)*100:.1f}% ({successes}/{episodes})")
    print(f"R√©compense moyenne: {np.mean(total_rewards):.2f} ¬± {np.std(total_rewards):.2f}")
    print(f"Pas moyens: {np.mean(total_steps_list):.1f} ¬± {np.std(total_steps_list):.1f}")
    print(f"{'='*80}\n")


def analyze_environment():
    """Analyse d√©taill√©e de l'environnement MountainCar"""
    print(f"\n{'='*80}")
    print("Insights ANALYSE DE L'ENVIRONNEMENT MOUNTAINCAR-V0")
    print(f"{'='*80}\n")
    
    env = gym.make('MountainCar-v0')
    
    print("[LIST] INFORMATIONS G√âN√âRALES")
    print("-" * 80)
    print(f"Espace d'observation: {env.observation_space}")
    print(f"  - Position: [-1.2, 0.6]")
    print(f"  - Vitesse: [-0.07, 0.07]")
    print(f"\nEspace d'actions: {env.action_space}")
    print(f"  - 0: Acc√©l√®re vers la gauche")
    print(f"  - 1: Pas d'acc√©l√©ration")
    print(f"  - 2: Acc√©l√®re vers la droite")
    print(f"\nObjectif: Atteindre position >= 0.5 (drapeau)")
    print(f"R√©compense: -1 √† chaque pas (encourage rapidit√©)")
    print(f"Max steps: 200")
    
    print(f"\n{'='*80}")
    print("üß™ TEST DE L'ENVIRONNEMENT")
    print(f"{'='*80}\n")
    
    # Test de quelques actions
    state, _ = env.reset()
    print(f"√âtat initial: position={state[0]:.3f}, vitesse={state[1]:.3f}")
    
    actions = [2, 2, 2, 2, 2]  # Acc√©l√®re vers la droite
    print(f"\nTest: 5 actions vers la droite")
    for i, action in enumerate(actions):
        state, reward, terminated, truncated, _ = env.step(action)
        print(f"  Pas {i+1}: position={state[0]:.3f}, vitesse={state[1]:.3f}, "
              f"reward={reward:.0f}")
        if terminated or truncated:
            break
    
    # Reset et test actions gauche
    state, _ = env.reset()
    actions = [0, 0, 0, 0, 0]  # Acc√©l√®re vers la gauche
    print(f"\nTest: 5 actions vers la gauche")
    for i, action in enumerate(actions):
        state, reward, terminated, truncated, _ = env.step(action)
        print(f"  Pas {i+1}: position={state[0]:.3f}, vitesse={state[1]:.3f}, "
              f"reward={reward:.0f}")
        if terminated or truncated:
            break
    
    print(f"\n[INFO] STRAT√âGIE OPTIMALE:")
    print("-" * 80)
    print("La voiture doit prendre de l'√©lan en oscillant entre les deux collines")
    print("pour accumuler assez de vitesse et atteindre le drapeau.")
    print("Actions directes vers la droite ne suffisent g√©n√©ralement pas!")
    print(f"{'='*80}\n")
    
    env.close()


def compare_strategies():
    """Compare diff√©rentes strat√©gies simples"""
    print(f"\n{'='*80}")
    print("[COMPARE]  COMPARAISON DE STRAT√âGIES SIMPLES")
    print(f"{'='*80}\n")
    
    env = gym.make('MountainCar-v0')
    
    strategies = {
        'Toujours droite': lambda state: 2,
        'Toujours gauche': lambda state: 0,
        'Selon vitesse': lambda state: 2 if state[1] >= 0 else 0,
        'Oscillation': lambda state: 2 if state[0] >= -0.5 else 0,
    }
    
    for name, strategy in strategies.items():
        rewards = []
        successes = 0
        
        for _ in range(10):
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            
            while steps < 200:
                action = strategy(state)
                state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                steps += 1
                
                if terminated or truncated:
                    break
            
            rewards.append(total_reward)
            if state[0] >= 0.5:
                successes += 1
        
        print(f"[PLOT] {name:20} | Succ√®s: {successes:2}/10 | "
              f"R√©compense moy: {np.mean(rewards):7.2f}")
    
    print(f"\n{'='*80}\n")
    env.close()


def main():
    """Script principal de d√©monstration"""
    print(f"\n{'='*80}")
    print("üéÆ D√âMONSTRATION MOUNTAINCAR-V0")
    print(f"{'='*80}\n")
    
    print("Choisissez une option:")
    print("1. Analyser l'environnement")
    print("2. Voir un agent al√©atoire")
    print("3. Voir l'agent entra√Æn√©")
    print("4. Comparer des strat√©gies simples")
    print("5. Tout ex√©cuter")
    print("0. Quitter")
    
    choice = input("\nVotre choix (0-5): ").strip()
    
    if choice == '1':
        analyze_environment()
    elif choice == '2':
        watch_random_agent(episodes=3)
    elif choice == '3':
        agent_path = "results/mountain_car_agent_classical.pkl"
        watch_trained_agent(agent_path, episodes=5)
    elif choice == '4':
        compare_strategies()
    elif choice == '5':
        analyze_environment()
        input("\nAppuyez sur Entr√©e pour voir l'agent al√©atoire...")
        watch_random_agent(episodes=2)
        input("\nAppuyez sur Entr√©e pour comparer les strat√©gies...")
        compare_strategies()
        agent_path = "results/mountain_car_agent_classical.pkl"
        if os.path.exists(agent_path):
            input("\nAppuyez sur Entr√©e pour voir l'agent entra√Æn√©...")
            watch_trained_agent(agent_path, episodes=3)
        else:
            print("\n[WARN]  Agent entra√Æn√© non trouv√©. Ex√©cutez d'abord:")
            print("   python train_mountaincar_classical.py")
    elif choice == '0':
        print("Au revoir! üëã")
    else:
        print("[ERROR] Choix invalide")


if __name__ == "__main__":
    main()
