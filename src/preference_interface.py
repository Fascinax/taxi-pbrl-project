from typing import Tuple, List, Dict, Any
import json
import os
from datetime import datetime
from src.trajectory_manager import Trajectory, TrajectoryManager
from src.visual_trajectory_comparator import VisualTrajectoryComparator

class PreferenceInterface:
    """
    Interface pour collecter les pr√©f√©rences entre trajectoires
    """
    
    def __init__(self):
        self.preferences: List[Dict[str, Any]] = []
        self.preference_counter = 0
    
    def collect_preference_interactive(self, traj1: Trajectory, traj2: Trajectory, 
                                     trajectory_manager: TrajectoryManager,
                                     use_visual: bool = True) -> int:
        """
        Interface interactive pour collecter une pr√©f√©rence entre deux trajectoires
        
        Args:
            traj1: Premi√®re trajectoire
            traj2: Deuxi√®me trajectoire
            trajectory_manager: Manager pour affichage des trajectoires
            use_visual: Si True, affiche la visualisation graphique Gymnasium avant le choix
            
        Returns:
            int: Choix de l'utilisateur (1 pour traj1, 2 pour traj2, 0 pour √©galit√©)
        """
        print("\n" + "="*100)
        print("[AGENT] SYST√àME DE PR√âF√âRENCES - √âVALUATION DE TRAJECTOIRES")
        print("="*100)
        
        # Visualisation graphique Gymnasium AVANT le choix
        if use_visual:
            try:
                print("\nüé¨ Pr√©paration de la visualisation graphique...")
                visualizer = VisualTrajectoryComparator()
                visualizer.replay_trajectories_side_by_side(traj1, traj2, delay=0.3)
                visualizer.close()
                print("‚úÖ Visualisation termin√©e!")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la visualisation: {e}")
                print("üìä Affichage de la comparaison textuelle √† la place...")
                trajectory_manager.display_trajectory_comparison(traj1, traj2)
        else:
            # Affichage textuel si visualisation d√©sactiv√©e
            trajectory_manager.display_trajectory_comparison(traj1, traj2)
        
        # Demande de pr√©f√©rence dans le terminal
        print("\n" + "üî• VOTRE CHOIX:")
        print("Quelle trajectoire pr√©f√©rez-vous ?")
        print("[INFO] Crit√®res √† consid√©rer: efficacit√©, r√©compense, style de navigation, succ√®s...")
        print("")
        print("1  - Je pr√©f√®re la TRAJECTOIRE A")
        print("2  - Je pr√©f√®re la TRAJECTOIRE B") 
        print("0  - Les deux sont √©quivalentes (√©galit√©)")
        print("üÜò - Tapez 'help' pour plus d'informations")
        print("[TARGET] - Tapez 'viz' pour voir la visualisation graphique")
        print("")
        
        while True:
            try:
                choice = input("üëâ Votre choix (1/2/0/help/replay/text): ").strip().lower()
                
                if choice == 'help':
                    self._display_help()
                    continue
                elif choice == 'replay':
                    # Rejouer la visualisation
                    try:
                        visualizer = VisualTrajectoryComparator()
                        visualizer.replay_trajectories_side_by_side(traj1, traj2, delay=0.3)
                        visualizer.close()
                    except Exception as e:
                        print(f"‚ö†Ô∏è Erreur lors de la visualisation: {e}")
                    continue
                elif choice == 'text':
                    # Afficher la comparaison textuelle
                    trajectory_manager.display_trajectory_comparison(traj1, traj2)
                    continue
                elif choice in ['1', '2', '0']:
                    choice_int = int(choice)
                    
                    # Demande de justification (optionnelle)
                    reasoning = input("üí≠ Pourquoi ce choix ? (optionnel): ").strip()
                    if not reasoning:
                        reasoning = "Aucune justification fournie"
                    
                    # Enregistrement de la pr√©f√©rence
                    preference = self._record_preference(traj1, traj2, choice_int, reasoning)
                    
                    # Confirmation
                    choice_names = {1: "Trajectoire A", 2: "Trajectoire B", 0: "√âgalit√©"}
                    print(f"\n[OK] Pr√©f√©rence enregistr√©e: {choice_names[choice_int]}")
                    if reasoning != "Aucune justification fournie":
                        print(f"üìù Justification: {reasoning}")
                    
                    return choice_int
                else:
                    print("[ERROR] Choix invalide. Utilisez 1, 2, 0, 'help' ou 'viz'")
                    
            except (ValueError, KeyboardInterrupt):
                print("[ERROR] Entr√©e invalide ou interruption. R√©essayez.")
    
    def collect_preference_batch(self, trajectory_pairs: List[Tuple[Trajectory, Trajectory]], 
                               trajectory_manager: TrajectoryManager) -> List[int]:
        """
        Collecte des pr√©f√©rences pour plusieurs paires de trajectoires
        
        Args:
            trajectory_pairs: Liste de paires de trajectoires
            trajectory_manager: Manager pour affichage
            
        Returns:
            List[int]: Liste des pr√©f√©rences
        """
        preferences = []
        total_pairs = len(trajectory_pairs)
        
        print(f"\n[TARGET] SESSION DE PR√âF√âRENCES: {total_pairs} comparaisons √† effectuer")
        
        for i, (traj1, traj2) in enumerate(trajectory_pairs):
            print(f"\n[PLOT] Comparaison {i+1}/{total_pairs}")
            preference = self.collect_preference_interactive(traj1, traj2, trajectory_manager)
            preferences.append(preference)
            
            # Demande de continuation pour les sessions longues
            if i < total_pairs - 1 and (i + 1) % 5 == 0:
                continue_choice = input(f"\n[PAUSE]  Pause apr√®s {i+1} comparaisons. Continuer ? (y/n): ").strip().lower()
                if continue_choice in ['n', 'non', 'no']:
                    print(f"üõë Session interrompue. {i+1} pr√©f√©rences collect√©es.")
                    break
        
        self._save_session_summary(preferences, total_pairs)
        return preferences
    
    def _display_help(self):
        """Affiche l'aide pour le syst√®me de pr√©f√©rences"""
        print("\n" + "üÜò AIDE - SYST√àME DE PR√âF√âRENCES")
        print("-" * 50)
        print("[TARGET] OBJECTIF: Vous aidez l'agent √† apprendre vos pr√©f√©rences")
        print("   en comparant diff√©rentes fa√ßons de r√©soudre la t√¢che Taxi.")
        print("")
        print("[PLOT] CRIT√àRES DE COMPARAISON:")
        print("   ‚Ä¢ R√©compense totale: Plus √©lev√©e = mieux")
        print("   ‚Ä¢ Efficacit√©: R√©compense/pas de temps")
        print("   ‚Ä¢ Longueur √©pisode: Plus court peut √™tre mieux (mais pas toujours)")
        print("   ‚Ä¢ Style de navigation: Certains pr√©f√®rent des trajets directs")
        print("   ‚Ä¢ Succ√®s: L'agent a-t-il r√©ussi la t√¢che ?")
        print("")
        print("[INFO] CONSEILS:")
        print("   ‚Ä¢ Suivez votre intuition sur ce qui vous semble 'mieux'")
        print("   ‚Ä¢ Vous pouvez valoriser la s√©curit√©, la vitesse, l'√©l√©gance...")
        print("   ‚Ä¢ Les √©galit√©s (0) sont acceptables si vraiment √©quivalent")
        print("")
        print("üéÆ ACTIONS TAXI:")
        print("   ‚Ä¢ Sud/Nord/Est/Ouest: D√©placement")
        print("   ‚Ä¢ Prendre: R√©cup√©rer le passager")
        print("   ‚Ä¢ D√©poser: D√©poser le passager √† destination")
        print("-" * 50)
    
    def _record_preference(self, traj1: Trajectory, traj2: Trajectory, 
                          choice: int, reasoning: str) -> Dict[str, Any]:
        """
        Enregistre une pr√©f√©rence dans la base de donn√©es
        
        Args:
            traj1: Premi√®re trajectoire
            traj2: Deuxi√®me trajectoire  
            choice: Choix utilisateur (1, 2, ou 0)
            reasoning: Justification du choix
            
        Returns:
            Dict contenant la pr√©f√©rence enregistr√©e
        """
        preference = {
            'preference_id': self.preference_counter,
            'timestamp': datetime.now().isoformat(),
            'trajectory_a_id': traj1.episode_id,
            'trajectory_b_id': traj2.episode_id,
            'trajectory_a_reward': traj1.total_reward,
            'trajectory_b_reward': traj2.total_reward,
            'trajectory_a_length': traj1.episode_length,
            'trajectory_b_length': traj2.episode_length,
            'choice': choice,  # 1=traj1, 2=traj2, 0=equal
            'reasoning': reasoning,
            'trajectory_a_efficiency': traj1.total_reward / traj1.episode_length if traj1.episode_length > 0 else 0,
            'trajectory_b_efficiency': traj2.total_reward / traj2.episode_length if traj2.episode_length > 0 else 0
        }
        
        self.preferences.append(preference)
        self.preference_counter += 1
        
        return preference
    
    def _save_session_summary(self, preferences: List[int], total_pairs: int):
        """Sauvegarde un r√©sum√© de la session"""
        completed = len(preferences)
        choice_counts = {0: 0, 1: 0, 2: 0}
        for choice in preferences:
            choice_counts[choice] += 1
        
        print(f"\n[LIST] R√âSUM√â DE SESSION:")
        print(f"   ‚Ä¢ Comparaisons compl√©t√©es: {completed}/{total_pairs}")
        print(f"   ‚Ä¢ Pr√©f√©rences Trajectoire A: {choice_counts[1]}")
        print(f"   ‚Ä¢ Pr√©f√©rences Trajectoire B: {choice_counts[2]}")
        print(f"   ‚Ä¢ √âgalit√©s: {choice_counts[0]}")
        
    def get_preference_statistics(self) -> Dict[str, Any]:
        """
        Calcule des statistiques sur les pr√©f√©rences collect√©es
        
        Returns:
            Dict contenant les statistiques
        """
        if not self.preferences:
            return {"error": "Aucune pr√©f√©rence collect√©e"}
        
        total_prefs = len(self.preferences)
        choice_counts = {0: 0, 1: 0, 2: 0}
        
        for pref in self.preferences:
            choice_counts[pref['choice']] += 1
        
        # Analyse des crit√®res de pr√©f√©rence bas√©s sur les donn√©es
        reward_preferences = []  # Pr√©f√®re-t-on les r√©compenses √©lev√©es ?
        efficiency_preferences = []  # Pr√©f√®re-t-on l'efficacit√© ?
        
        for pref in self.preferences:
            if pref['choice'] in [1, 2]:  # Pas d'√©galit√©
                preferred_traj = 'a' if pref['choice'] == 1 else 'b'
                
                # Comparaison r√©compenses
                reward_a = pref['trajectory_a_reward']
                reward_b = pref['trajectory_b_reward']
                if reward_a != reward_b:
                    prefers_higher_reward = (preferred_traj == 'a' and reward_a > reward_b) or \
                                          (preferred_traj == 'b' and reward_b > reward_a)
                    reward_preferences.append(prefers_higher_reward)
                
                # Comparaison efficacit√©
                eff_a = pref['trajectory_a_efficiency']
                eff_b = pref['trajectory_b_efficiency']
                if abs(eff_a - eff_b) > 0.01:  # Diff√©rence significative
                    prefers_higher_efficiency = (preferred_traj == 'a' and eff_a > eff_b) or \
                                              (preferred_traj == 'b' and eff_b > eff_a)
                    efficiency_preferences.append(prefers_higher_efficiency)
        
        stats = {
            'total_preferences': total_prefs,
            'choice_distribution': {
                'trajectory_a': choice_counts[1],
                'trajectory_b': choice_counts[2], 
                'equal': choice_counts[0]
            },
            'choice_percentages': {
                'trajectory_a': (choice_counts[1] / total_prefs * 100) if total_prefs > 0 else 0,
                'trajectory_b': (choice_counts[2] / total_prefs * 100) if total_prefs > 0 else 0,
                'equal': (choice_counts[0] / total_prefs * 100) if total_prefs > 0 else 0
            },
            'reward_preference_tendency': sum(reward_preferences) / len(reward_preferences) * 100 if reward_preferences else 0,
            'efficiency_preference_tendency': sum(efficiency_preferences) / len(efficiency_preferences) * 100 if efficiency_preferences else 0
        }
        
        return stats
    
    def save_preferences(self, filepath: str):
        """Sauvegarde les pr√©f√©rences dans un fichier JSON"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        data = {
            'preferences': self.preferences,
            'statistics': self.get_preference_statistics(),
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'total_preferences': len(self.preferences)
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"Pr√©f√©rences sauvegard√©es: {filepath}")
    
    def load_preferences(self, filepath: str):
        """Charge les pr√©f√©rences depuis un fichier JSON"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.preferences = data['preferences']
        self.preference_counter = len(self.preferences)
        
        print(f"Pr√©f√©rences charg√©es: {filepath}")
        
    def display_preferences_summary(self):
        """Affiche un r√©sum√© des pr√©f√©rences collect√©es"""
        stats = self.get_preference_statistics()
        
        if 'error' in stats:
            print(stats['error'])
            return
        
        print("\n" + "="*60)
        print("[PLOT] R√âSUM√â DES PR√âF√âRENCES COLLECT√âES")
        print("="*60)
        print(f"Total de pr√©f√©rences: {stats['total_preferences']}")
        print(f"Trajectoire A pr√©f√©r√©e: {stats['choice_distribution']['trajectory_a']} fois ({stats['choice_percentages']['trajectory_a']:.1f}%)")
        print(f"Trajectoire B pr√©f√©r√©e: {stats['choice_distribution']['trajectory_b']} fois ({stats['choice_percentages']['trajectory_b']:.1f}%)")
        print(f"√âgalit√©s d√©clar√©es: {stats['choice_distribution']['equal']} fois ({stats['choice_percentages']['equal']:.1f}%)")
        
        print(f"\n[TARGET] TENDANCES D√âTECT√âES:")
        print(f"Pr√©f√©rence pour r√©compenses √©lev√©es: {stats['reward_preference_tendency']:.1f}%")
        print(f"Pr√©f√©rence pour efficacit√© √©lev√©e: {stats['efficiency_preference_tendency']:.1f}%")
        print("="*60)