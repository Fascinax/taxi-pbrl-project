from typing import Tuple, List, Dict, Any
import json
import os
from datetime import datetime
from src.trajectory_manager import Trajectory, TrajectoryManager
from src.visual_trajectory_comparator import VisualTrajectoryComparator

class PreferenceInterface:
    """
    Interface pour collecter les prÃ©fÃ©rences entre trajectoires
    """
    
    def __init__(self):
        self.preferences: List[Dict[str, Any]] = []
        self.preference_counter = 0
    
    def collect_preference_interactive(self, traj1: Trajectory, traj2: Trajectory, 
                                     trajectory_manager: TrajectoryManager, 
                                     use_visual: bool = True) -> int:
        """
        Interface interactive pour collecter une prÃ©fÃ©rence entre deux trajectoires
        
        Args:
            traj1: PremiÃ¨re trajectoire
            traj2: DeuxiÃ¨me trajectoire
            trajectory_manager: Manager pour affichage des trajectoires
            use_visual: Si True, affiche la visualisation Gymnasium (dÃ©faut: True)
            
        Returns:
            int: Choix de l'utilisateur (1 pour traj1, 2 pour traj2, 0 pour Ã©galitÃ©)
        """
        print("\n" + "="*100)
        print("ğŸ¤– SYSTÃˆME DE PRÃ‰FÃ‰RENCES - Ã‰VALUATION DE TRAJECTOIRES")
        print("="*100)
        
        # Visualisation graphique Gymnasium (par dÃ©faut)
        if use_visual:
            try:
                visualizer = VisualTrajectoryComparator()
                visualizer.replay_trajectories_side_by_side(traj1, traj2, delay=0.3)
                visualizer.close()
            except Exception as e:
                print(f"âš ï¸ Erreur lors de la visualisation graphique: {e}")
                print("ğŸ“Š Passage Ã  l'affichage textuel...")
                use_visual = False
        
        # Affichage de la comparaison textuelle (toujours disponible)
        if not use_visual:
            trajectory_manager.display_trajectory_comparison(traj1, traj2)
        
        # Demande de prÃ©fÃ©rence
        print("\n" + "ğŸ”¥ VOTRE CHOIX:")
        print("Quelle trajectoire prÃ©fÃ©rez-vous ?")
        print("ğŸ’¡ CritÃ¨res Ã  considÃ©rer: efficacitÃ©, rÃ©compense, style de navigation, succÃ¨s...")
        print("")
        print("1ï¸âƒ£  - Je prÃ©fÃ¨re la TRAJECTOIRE A (cyan)")
        print("2ï¸âƒ£  - Je prÃ©fÃ¨re la TRAJECTOIRE B (orange)") 
        print("0ï¸âƒ£  - Les deux sont Ã©quivalentes (Ã©galitÃ©)")
        print("ğŸ†˜ - Tapez 'help' pour plus d'informations")
        print("ğŸ¯ - Tapez 'replay' pour revoir la visualisation graphique")
        print("ğŸ“Š - Tapez 'text' pour voir la comparaison textuelle dÃ©taillÃ©e")
        print("")
        
        while True:
            try:
                choice = input("ğŸ‘‰ Votre choix (1/2/0/help/replay/text): ").strip().lower()
                
                if choice == 'help':
                    self._display_help()
                    continue
                elif choice == 'replay':
                    # Rejouer la visualisation Gymnasium
                    try:
                        visualizer = VisualTrajectoryComparator()
                        visualizer.replay_trajectories_side_by_side(traj1, traj2, delay=0.3)
                        visualizer.close()
                    except Exception as e:
                        print(f"âš ï¸ Erreur lors de la visualisation: {e}")
                    continue
                elif choice == 'text':
                    # Afficher la comparaison textuelle dÃ©taillÃ©e
                    trajectory_manager.display_trajectory_comparison(traj1, traj2)
                    continue
                elif choice == 'viz':
                    # RÃ©trocompatibilitÃ© avec l'ancien 'viz'
                    trajectory_manager.visualize_trajectories(traj1, traj2)
                    continue
                elif choice in ['1', '2', '0']:
                    choice_int = int(choice)
                    
                    # Demande de justification (optionnelle)
                    reasoning = input("ğŸ’­ Pourquoi ce choix ? (optionnel): ").strip()
                    if not reasoning:
                        reasoning = "Aucune justification fournie"
                    
                    # Enregistrement de la prÃ©fÃ©rence
                    preference = self._record_preference(traj1, traj2, choice_int, reasoning)
                    
                    # Confirmation
                    choice_names = {1: "Trajectoire A", 2: "Trajectoire B", 0: "Ã‰galitÃ©"}
                    print(f"\nâœ… PrÃ©fÃ©rence enregistrÃ©e: {choice_names[choice_int]}")
                    if reasoning != "Aucune justification fournie":
                        print(f"ğŸ“ Justification: {reasoning}")
                    
                    return choice_int
                else:
                    print("âŒ Choix invalide. Utilisez 1, 2, 0, 'help', 'replay' ou 'text'")
                    
            except (ValueError, KeyboardInterrupt):
                print("âŒ EntrÃ©e invalide ou interruption. RÃ©essayez.")
    
    def collect_preference_batch(self, trajectory_pairs: List[Tuple[Trajectory, Trajectory]], 
                               trajectory_manager: TrajectoryManager) -> List[int]:
        """
        Collecte des prÃ©fÃ©rences pour plusieurs paires de trajectoires
        
        Args:
            trajectory_pairs: Liste de paires de trajectoires
            trajectory_manager: Manager pour affichage
            
        Returns:
            List[int]: Liste des prÃ©fÃ©rences
        """
        preferences = []
        total_pairs = len(trajectory_pairs)
        
        print(f"\nğŸ¯ SESSION DE PRÃ‰FÃ‰RENCES: {total_pairs} comparaisons Ã  effectuer")
        
        for i, (traj1, traj2) in enumerate(trajectory_pairs):
            print(f"\nğŸ“Š Comparaison {i+1}/{total_pairs}")
            preference = self.collect_preference_interactive(traj1, traj2, trajectory_manager)
            preferences.append(preference)
            
            # Demande de continuation pour les sessions longues
            if i < total_pairs - 1 and (i + 1) % 5 == 0:
                continue_choice = input(f"\nâ¸ï¸  Pause aprÃ¨s {i+1} comparaisons. Continuer ? (y/n): ").strip().lower()
                if continue_choice in ['n', 'non', 'no']:
                    print(f"ğŸ›‘ Session interrompue. {i+1} prÃ©fÃ©rences collectÃ©es.")
                    break
        
        self._save_session_summary(preferences, total_pairs)
        return preferences
    
    def _display_help(self):
        """Affiche l'aide pour le systÃ¨me de prÃ©fÃ©rences"""
        print("\n" + "ğŸ†˜ AIDE - SYSTÃˆME DE PRÃ‰FÃ‰RENCES")
        print("-" * 50)
        print("ğŸ¯ OBJECTIF: Vous aidez l'agent Ã  apprendre vos prÃ©fÃ©rences")
        print("   en comparant diffÃ©rentes faÃ§ons de rÃ©soudre la tÃ¢che Taxi.")
        print("")
        print("ğŸ“Š CRITÃˆRES DE COMPARAISON:")
        print("   â€¢ RÃ©compense totale: Plus Ã©levÃ©e = mieux")
        print("   â€¢ EfficacitÃ©: RÃ©compense/pas de temps")
        print("   â€¢ Longueur Ã©pisode: Plus court peut Ãªtre mieux (mais pas toujours)")
        print("   â€¢ Style de navigation: Certains prÃ©fÃ¨rent des trajets directs")
        print("   â€¢ SuccÃ¨s: L'agent a-t-il rÃ©ussi la tÃ¢che ?")
        print("")
        print("ğŸ’¡ CONSEILS:")
        print("   â€¢ Suivez votre intuition sur ce qui vous semble 'mieux'")
        print("   â€¢ Vous pouvez valoriser la sÃ©curitÃ©, la vitesse, l'Ã©lÃ©gance...")
        print("   â€¢ Les Ã©galitÃ©s (0) sont acceptables si vraiment Ã©quivalent")
        print("")
        print("ğŸ® ACTIONS TAXI:")
        print("   â€¢ Sud/Nord/Est/Ouest: DÃ©placement")
        print("   â€¢ Prendre: RÃ©cupÃ©rer le passager")
        print("   â€¢ DÃ©poser: DÃ©poser le passager Ã  destination")
        print("-" * 50)
    
    def _record_preference(self, traj1: Trajectory, traj2: Trajectory, 
                          choice: int, reasoning: str) -> Dict[str, Any]:
        """
        Enregistre une prÃ©fÃ©rence dans la base de donnÃ©es
        
        Args:
            traj1: PremiÃ¨re trajectoire
            traj2: DeuxiÃ¨me trajectoire  
            choice: Choix utilisateur (1, 2, ou 0)
            reasoning: Justification du choix
            
        Returns:
            Dict contenant la prÃ©fÃ©rence enregistrÃ©e
        """
        preference = {
            'preference_id': self.preference_counter,
            'timestamp': datetime.now().isoformat(),
            'trajectory_a_id': traj1.episode_id,
            'trajectory_b_id': traj2.episode_id,
            'trajectory_a_reward': traj1.total_reward,
            'trajectory_b_reward': traj2.total_reward,
            'trajectory_a_length': traj1.episode_length,
            'trajectory_b_length': traj2.episode_length,
            'choice': choice,  # 1=traj1, 2=traj2, 0=equal
            'reasoning': reasoning,
            'trajectory_a_efficiency': traj1.total_reward / traj1.episode_length if traj1.episode_length > 0 else 0,
            'trajectory_b_efficiency': traj2.total_reward / traj2.episode_length if traj2.episode_length > 0 else 0
        }
        
        self.preferences.append(preference)
        self.preference_counter += 1
        
        return preference
    
    def _save_session_summary(self, preferences: List[int], total_pairs: int):
        """Sauvegarde un rÃ©sumÃ© de la session"""
        completed = len(preferences)
        choice_counts = {0: 0, 1: 0, 2: 0}
        for choice in preferences:
            choice_counts[choice] += 1
        
        print(f"\nğŸ“‹ RÃ‰SUMÃ‰ DE SESSION:")
        print(f"   â€¢ Comparaisons complÃ©tÃ©es: {completed}/{total_pairs}")
        print(f"   â€¢ PrÃ©fÃ©rences Trajectoire A: {choice_counts[1]}")
        print(f"   â€¢ PrÃ©fÃ©rences Trajectoire B: {choice_counts[2]}")
        print(f"   â€¢ Ã‰galitÃ©s: {choice_counts[0]}")
        
    def get_preference_statistics(self) -> Dict[str, Any]:
        """
        Calcule des statistiques sur les prÃ©fÃ©rences collectÃ©es
        
        Returns:
            Dict contenant les statistiques
        """
        if not self.preferences:
            return {"error": "Aucune prÃ©fÃ©rence collectÃ©e"}
        
        total_prefs = len(self.preferences)
        choice_counts = {0: 0, 1: 0, 2: 0}
        
        for pref in self.preferences:
            choice_counts[pref['choice']] += 1
        
        # Analyse des critÃ¨res de prÃ©fÃ©rence basÃ©s sur les donnÃ©es
        reward_preferences = []  # PrÃ©fÃ¨re-t-on les rÃ©compenses Ã©levÃ©es ?
        efficiency_preferences = []  # PrÃ©fÃ¨re-t-on l'efficacitÃ© ?
        
        for pref in self.preferences:
            if pref['choice'] in [1, 2]:  # Pas d'Ã©galitÃ©
                preferred_traj = 'a' if pref['choice'] == 1 else 'b'
                
                # Comparaison rÃ©compenses
                reward_a = pref['trajectory_a_reward']
                reward_b = pref['trajectory_b_reward']
                if reward_a != reward_b:
                    prefers_higher_reward = (preferred_traj == 'a' and reward_a > reward_b) or \
                                          (preferred_traj == 'b' and reward_b > reward_a)
                    reward_preferences.append(prefers_higher_reward)
                
                # Comparaison efficacitÃ©
                eff_a = pref['trajectory_a_efficiency']
                eff_b = pref['trajectory_b_efficiency']
                if abs(eff_a - eff_b) > 0.01:  # DiffÃ©rence significative
                    prefers_higher_efficiency = (preferred_traj == 'a' and eff_a > eff_b) or \
                                              (preferred_traj == 'b' and eff_b > eff_a)
                    efficiency_preferences.append(prefers_higher_efficiency)
        
        stats = {
            'total_preferences': total_prefs,
            'choice_distribution': {
                'trajectory_a': choice_counts[1],
                'trajectory_b': choice_counts[2], 
                'equal': choice_counts[0]
            },
            'choice_percentages': {
                'trajectory_a': (choice_counts[1] / total_prefs * 100) if total_prefs > 0 else 0,
                'trajectory_b': (choice_counts[2] / total_prefs * 100) if total_prefs > 0 else 0,
                'equal': (choice_counts[0] / total_prefs * 100) if total_prefs > 0 else 0
            },
            'reward_preference_tendency': sum(reward_preferences) / len(reward_preferences) * 100 if reward_preferences else 0,
            'efficiency_preference_tendency': sum(efficiency_preferences) / len(efficiency_preferences) * 100 if efficiency_preferences else 0
        }
        
        return stats
    
    def save_preferences(self, filepath: str):
        """Sauvegarde les prÃ©fÃ©rences dans un fichier JSON"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        data = {
            'preferences': self.preferences,
            'statistics': self.get_preference_statistics(),
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'total_preferences': len(self.preferences)
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"PrÃ©fÃ©rences sauvegardÃ©es: {filepath}")
    
    def load_preferences(self, filepath: str):
        """Charge les prÃ©fÃ©rences depuis un fichier JSON"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.preferences = data['preferences']
        self.preference_counter = len(self.preferences)
        
        print(f"PrÃ©fÃ©rences chargÃ©es: {filepath}")
        
    def display_preferences_summary(self):
        """Affiche un rÃ©sumÃ© des prÃ©fÃ©rences collectÃ©es"""
        stats = self.get_preference_statistics()
        
        if 'error' in stats:
            print(stats['error'])
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š RÃ‰SUMÃ‰ DES PRÃ‰FÃ‰RENCES COLLECTÃ‰ES")
        print("="*60)
        print(f"Total de prÃ©fÃ©rences: {stats['total_preferences']}")
        print(f"Trajectoire A prÃ©fÃ©rÃ©e: {stats['choice_distribution']['trajectory_a']} fois ({stats['choice_percentages']['trajectory_a']:.1f}%)")
        print(f"Trajectoire B prÃ©fÃ©rÃ©e: {stats['choice_distribution']['trajectory_b']} fois ({stats['choice_percentages']['trajectory_b']:.1f}%)")
        print(f"Ã‰galitÃ©s dÃ©clarÃ©es: {stats['choice_distribution']['equal']} fois ({stats['choice_percentages']['equal']:.1f}%)")
        
        print(f"\nğŸ¯ TENDANCES DÃ‰TECTÃ‰ES:")
        print(f"PrÃ©fÃ©rence pour rÃ©compenses Ã©levÃ©es: {stats['reward_preference_tendency']:.1f}%")
        print(f"PrÃ©fÃ©rence pour efficacitÃ© Ã©levÃ©e: {stats['efficiency_preference_tendency']:.1f}%")
        print("="*60)