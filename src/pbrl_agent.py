import numpy as np
from typing import List, Dict, Tuple, Any
from src.q_learning_agent import QLearningAgent
from src.trajectory_manager import Trajectory, TrajectoryStep
from src.preference_interface import PreferenceInterface
import copy

class PreferenceBasedQLearning(QLearningAgent):
    """
    Agent Q-Learning modifi√© pour apprendre √† partir des pr√©f√©rences humaines
    Impl√©mente une version simplifi√©e du Preference-based Reinforcement Learning
    """
    
    def __init__(self, n_states: int, n_actions: int, 
                 learning_rate: float = 0.1, 
                 discount_factor: float = 0.95,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 preference_weight: float = 0.5):
        """
        Initialise l'agent PbRL
        
        Args:
            preference_weight: Poids donn√© aux signaux de pr√©f√©rence vs r√©compenses originales
        """
        super().__init__(n_states, n_actions, learning_rate, discount_factor,
                        epsilon, epsilon_decay, epsilon_min)
        
        self.preference_weight = preference_weight
        self.preference_rewards = {}  # Cache des r√©compenses calcul√©es √† partir des pr√©f√©rences
        self.trajectory_values = {}   # Valeurs apprises pour les trajectoires compl√®tes
        
        # Statistiques pour le suivi
        self.preference_updates = 0
        self.preference_learning_history = []
    
    def update_from_preferences(self, preferred_trajectory: Trajectory, 
                              less_preferred_trajectory: Trajectory,
                              preference_strength: float = 1.0):
        """
        Met √† jour la Q-table en fonction d'une pr√©f√©rence entre deux trajectoires
        
        Args:
            preferred_trajectory: Trajectoire pr√©f√©r√©e
            less_preferred_trajectory: Trajectoire moins pr√©f√©r√©e  
            preference_strength: Force de la pr√©f√©rence (0.5 = l√©g√®re, 1.0 = forte, 2.0 = tr√®s forte)
        """
        
        # Calcul du bonus/malus bas√© sur la pr√©f√©rence
        reward_bonus = preference_strength * self.preference_weight
        reward_penalty = -preference_strength * self.preference_weight * 0.5  # P√©nalit√© moins forte
        
        # Mise √† jour pour la trajectoire pr√©f√©r√©e (bonus)
        self._update_trajectory_values(preferred_trajectory, reward_bonus, is_preferred=True)
        
        # Mise √† jour pour la trajectoire moins pr√©f√©r√©e (p√©nalit√©)
        self._update_trajectory_values(less_preferred_trajectory, reward_penalty, is_preferred=False)
        
        self.preference_updates += 1
        
        # Enregistrement pour analyse
        self.preference_learning_history.append({
            'preferred_reward': preferred_trajectory.total_reward,
            'less_preferred_reward': less_preferred_trajectory.total_reward,
            'preference_strength': preference_strength,
            'reward_bonus': reward_bonus,
            'reward_penalty': reward_penalty
        })
    
    def _update_trajectory_values(self, trajectory: Trajectory, reward_modifier: float, 
                                is_preferred: bool):
        """
        Met √† jour les valeurs Q pour toutes les transitions d'une trajectoire
        
        Args:
            trajectory: Trajectoire √† mettre √† jour
            reward_modifier: Modification de r√©compense √† appliquer
            is_preferred: True si c'est la trajectoire pr√©f√©r√©e
        """
        
        # Mise √† jour de chaque transition dans la trajectoire
        for i, step in enumerate(trajectory.steps):
            # R√©compense modifi√©e bas√©e sur la pr√©f√©rence
            modified_reward = step.reward + reward_modifier
            
            # Facteur de diminution pour les actions en fin de trajectoire
            # (les derni√®res actions sont moins importantes)
            position_weight = 1.0 - (i / len(trajectory.steps)) * 0.3
            final_reward = modified_reward * position_weight
            
            # Mise √† jour Q-Learning standard avec la r√©compense modifi√©e
            if step.done:
                target = final_reward
            else:
                target = final_reward + self.gamma * np.max(self.q_table[step.next_state])
            
            # Mise √† jour avec un taux d'apprentissage r√©duit pour les pr√©f√©rences
            preference_lr = self.lr * 0.5  # Apprentissage plus conservateur
            self.q_table[step.state, step.action] += preference_lr * \
                (target - self.q_table[step.state, step.action])
    
    def train_with_preferences(self, env, trajectories: List[Trajectory], 
                             preferences: List[Dict[str, Any]], 
                             episodes: int = 5000) -> List[float]:
        """
        Entra√Æne l'agent en combinant exploration normale et apprentissage par pr√©f√©rences
        
        Args:
            env: Environnement Gymnasium
            trajectories: Liste des trajectoires pour les pr√©f√©rences
            preferences: Liste des pr√©f√©rences collect√©es
            episodes: Nombre d'√©pisodes d'entra√Ænement
            
        Returns:
            Liste des r√©compenses par √©pisode
        """
        episode_rewards = []
        
        print(f"Entra√Ænement PbRL: {episodes} √©pisodes avec {len(preferences)} pr√©f√©rences")
        
        # Phase 1: Apprentissage initial par pr√©f√©rences
        print("Phase 1: Application des pr√©f√©rences existantes...")
        self._apply_existing_preferences(trajectories, preferences)
        
        # Phase 2: Entra√Ænement normal avec Q-table modifi√©e
        print("Phase 2: Entra√Ænement avec exploration...")
        for episode in range(episodes):
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            max_steps = 200
            
            while steps < max_steps:
                # S√©lection et ex√©cution de l'action
                action = self.select_action(state, training=True)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                # Mise √† jour normale de la Q-table
                self.update_q_table(state, action, reward, next_state, done)
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # Mise √† jour d'epsilon
            self.decay_epsilon()
            
            episode_rewards.append(total_reward)
            
            # Affichage du progr√®s
            if (episode + 1) % 1000 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"√âpisode {episode + 1}/{episodes}, "
                      f"R√©compense moyenne (100 derniers): {avg_reward:.2f}, "
                      f"Epsilon: {self.epsilon:.3f}")
        
        self.training_rewards = episode_rewards
        return episode_rewards
    
    def _apply_existing_preferences(self, trajectories: List[Trajectory], 
                                  preferences: List[Dict[str, Any]]):
        """
        Applique les pr√©f√©rences existantes pour initialiser la Q-table
        """
        
        # Cr√©er un mapping des trajectoires par ID
        traj_dict = {traj.episode_id: traj for traj in trajectories}
        
        for pref in preferences:
            if pref['choice'] == 0:  # √âgalit√©, on ignore
                continue
            
            traj_a_id = pref['trajectory_a_id']
            traj_b_id = pref['trajectory_b_id']
            
            if traj_a_id not in traj_dict or traj_b_id not in traj_dict:
                continue
            
            traj_a = traj_dict[traj_a_id]
            traj_b = traj_dict[traj_b_id]
            
            # D√©terminer quelle trajectoire est pr√©f√©r√©e
            if pref['choice'] == 1:  # Trajectoire A pr√©f√©r√©e
                preferred = traj_a
                less_preferred = traj_b
            else:  # Trajectoire B pr√©f√©r√©e
                preferred = traj_b
                less_preferred = traj_a
            
            # Calculer la force de pr√©f√©rence bas√©e sur la diff√©rence de performance
            reward_diff = abs(preferred.total_reward - less_preferred.total_reward)
            efficiency_diff = abs(pref['trajectory_a_efficiency'] - pref['trajectory_b_efficiency'])
            
            # Force adaptative bas√©e sur les diff√©rences
            strength = 1.0 + min(reward_diff / 10.0, 1.0) + min(efficiency_diff, 0.5)
            
            # Appliquer la pr√©f√©rence
            self.update_from_preferences(preferred, less_preferred, strength)
        
        print(f"‚úÖ {len([p for p in preferences if p['choice'] != 0])} pr√©f√©rences appliqu√©es")
    
    def interactive_training_loop(self, env, preference_interface: PreferenceInterface,
                                trajectory_manager, episodes_per_iteration: int = 1000,
                                max_iterations: int = 5, trajectories_per_comparison: int = 5):
        """
        Boucle d'entra√Ænement interactive avec collecte de pr√©f√©rences en temps r√©el
        
        Args:
            env: Environnement
            preference_interface: Interface de collecte de pr√©f√©rences
            trajectory_manager: Gestionnaire de trajectoires
            episodes_per_iteration: √âpisodes entre chaque collecte de pr√©f√©rences
            max_iterations: Nombre maximum d'it√©rations
            trajectories_per_comparison: Nombre de trajectoires √† g√©n√©rer pour comparaison
        """
        
        print(f"üöÄ ENTRA√éNEMENT INTERACTIF PbRL")
        print(f"Param√®tres: {max_iterations} it√©rations, {episodes_per_iteration} √©pisodes/it√©ration")
        
        all_rewards = []
        iteration_summaries = []
        
        for iteration in range(max_iterations):
            print(f"\nüîÑ IT√âRATION {iteration + 1}/{max_iterations}")
            
            # 1. Entra√Ænement standard
            print(f"1Ô∏è‚É£ Entra√Ænement standard ({episodes_per_iteration} √©pisodes)...")
            iteration_rewards = self.train(env, episodes=episodes_per_iteration)
            all_rewards.extend(iteration_rewards)
            
            # 2. G√©n√©ration de trajectoires pour comparaison
            print(f"2Ô∏è‚É£ G√©n√©ration de {trajectories_per_comparison} trajectoires de test...")
            test_trajectories = []
            for i in range(trajectories_per_comparison):
                traj = trajectory_manager.collect_trajectory(env, self, render=False)
                test_trajectories.append(traj)
            
            # 3. S√©lection de paires int√©ressantes
            print("3Ô∏è‚É£ S√©lection de paires pour comparaison...")
            pairs = self._select_interesting_pairs(test_trajectories)
            
            if not pairs:
                print("‚ö†Ô∏è Aucune paire int√©ressante trouv√©e, passage √† l'it√©ration suivante")
                continue
            
            # 4. Collecte de pr√©f√©rences
            print(f"4Ô∏è‚É£ Collecte de pr√©f√©rences ({len(pairs)} comparaisons)...")
            preferences = preference_interface.collect_preference_batch(
                pairs, trajectory_manager, 
                env=env, 
                visual_replay=True
            )
            
            # 5. Application des nouvelles pr√©f√©rences
            print("5Ô∏è‚É£ Application des nouvelles pr√©f√©rences...")
            self._apply_new_preferences(pairs, preferences)
            
            # 6. R√©sum√© de l'it√©ration
            iteration_summary = {
                'iteration': iteration + 1,
                'avg_reward': np.mean(iteration_rewards),
                'preferences_collected': len([p for p in preferences if p != 0]),
                'total_preference_updates': self.preference_updates
            }
            iteration_summaries.append(iteration_summary)
            
            print(f"üìä R√©sum√© it√©ration {iteration + 1}:")
            print(f"   R√©compense moyenne: {iteration_summary['avg_reward']:.2f}")
            print(f"   Pr√©f√©rences collect√©es: {iteration_summary['preferences_collected']}")
            
        # R√©sum√© final
        print(f"\nüéâ ENTRA√éNEMENT INTERACTIF TERMIN√â")
        print(f"Total des mises √† jour par pr√©f√©rences: {self.preference_updates}")
        print(f"R√©compense finale moyenne: {np.mean(all_rewards[-100:]):.2f}")
        
        return all_rewards, iteration_summaries
    
    def _select_interesting_pairs(self, trajectories: List[Trajectory]) -> List[Tuple[Trajectory, Trajectory]]:
        """S√©lectionne des paires int√©ressantes pour la comparaison"""
        if len(trajectories) < 2:
            return []
        
        # Tri par performance pour cr√©er des contrastes
        sorted_trajs = sorted(trajectories, key=lambda t: t.total_reward, reverse=True)
        
        pairs = []
        
        # Paire best vs worst si diff√©rence significative
        if len(sorted_trajs) >= 2:
            best = sorted_trajs[0]
            worst = sorted_trajs[-1]
            if abs(best.total_reward - worst.total_reward) > 2:  # Diff√©rence significative
                pairs.append((best, worst))
        
        # Paire avec r√©compenses similaires mais efficacit√©s diff√©rentes
        middle_idx = len(sorted_trajs) // 2
        if middle_idx > 0 and middle_idx < len(sorted_trajs) - 1:
            traj1 = sorted_trajs[middle_idx]
            traj2 = sorted_trajs[middle_idx + 1]
            # V√©rifier si diff√©rence d'efficacit√© significative
            eff1 = traj1.total_reward / traj1.episode_length
            eff2 = traj2.total_reward / traj2.episode_length
            if abs(eff1 - eff2) > 0.1:
                pairs.append((traj1, traj2))
        
        return pairs
    
    def _apply_new_preferences(self, pairs: List[Tuple[Trajectory, Trajectory]], 
                             preferences: List[int]):
        """Applique les nouvelles pr√©f√©rences collect√©es"""
        
        for (traj1, traj2), preference in zip(pairs, preferences):
            if preference == 0:  # √âgalit√©
                continue
            elif preference == 1:  # Pr√©f√®re traj1
                self.update_from_preferences(traj1, traj2, preference_strength=1.0)
            elif preference == 2:  # Pr√©f√®re traj2
                self.update_from_preferences(traj2, traj1, preference_strength=1.0)
    
    def get_preference_learning_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© de l'apprentissage par pr√©f√©rences"""
        
        if not self.preference_learning_history:
            return {"error": "Aucun apprentissage par pr√©f√©rences effectu√©"}
        
        history = self.preference_learning_history
        
        return {
            'total_preference_updates': self.preference_updates,
            'avg_preference_strength': np.mean([h['preference_strength'] for h in history]),
            'avg_reward_bonus': np.mean([h['reward_bonus'] for h in history]),
            'avg_reward_penalty': np.mean([h['reward_penalty'] for h in history]),
            'reward_differences': [h['preferred_reward'] - h['less_preferred_reward'] for h in history],
            'preference_weight_used': self.preference_weight
        }
    
    def save_pbrl_agent(self, filepath: str):
        """Sauvegarde l'agent PbRL avec ses donn√©es sp√©cifiques"""
        save_data = {
            'q_table': self.q_table,
            'training_rewards': self.training_rewards,
            'preference_updates': self.preference_updates,
            'preference_learning_history': self.preference_learning_history,
            'preference_weight': self.preference_weight,
            'hyperparameters': {
                'lr': self.lr,
                'gamma': self.gamma,
                'epsilon_decay': self.epsilon_decay,
                'epsilon_min': self.epsilon_min,
                'preference_weight': self.preference_weight
            }
        }
        
        import os
        import pickle
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        print(f"Agent PbRL sauvegard√©: {filepath}")