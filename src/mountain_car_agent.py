"""
Agent Q-Learning adapt√© pour MountainCar-v0 avec discr√©tisation d'√©tats
"""

import numpy as np
import gymnasium as gym
from typing import List, Tuple, Optional
from src.q_learning_agent import QLearningAgent
from src.mountain_car_discretizer import MountainCarDiscretizer


class MountainCarAgent(QLearningAgent):
    """
    Agent Q-Learning sp√©cialis√© pour MountainCar avec discr√©tisation d'√©tats continus
    """
    
    def __init__(self, 
                 n_position_bins: int = 20,
                 n_velocity_bins: int = 20,
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.999,
                 epsilon_min: float = 0.01):
        """
        Initialise l'agent MountainCar
        
        Args:
            n_position_bins: Nombre de bins pour discr√©tiser la position
            n_velocity_bins: Nombre de bins pour discr√©tiser la vitesse
        """
        
        # Initialisation du discr√©tiseur
        self.discretizer = MountainCarDiscretizer(n_position_bins, n_velocity_bins)
        
        # Initialisation de l'agent Q-Learning parent
        n_states = self.discretizer.n_states
        n_actions = 3  # MountainCar: 0=gauche, 1=rien, 2=droite
        
        super().__init__(
            n_states=n_states,
            n_actions=n_actions,
            learning_rate=learning_rate,
            discount_factor=discount_factor,
            epsilon=epsilon,
            epsilon_decay=epsilon_decay,
            epsilon_min=epsilon_min
        )
        
        print(f"üöó MountainCarAgent initialis√©:")
        print(f"   - √âtats discrets: {n_states}")
        print(f"   - Actions: {n_actions} (0=gauche, 1=rien, 2=droite)")
        print(f"   - Learning rate: {learning_rate}")
        print(f"   - Gamma: {discount_factor}")
    
    def process_state(self, continuous_state: np.ndarray) -> int:
        """
        Convertit un √©tat continu en √©tat discret
        
        Args:
            continuous_state: √âtat [position, vitesse]
            
        Returns:
            √âtat discret (index)
        """
        return self.discretizer.discretize(continuous_state)
    
    def select_action(self, continuous_state: np.ndarray, training: bool = True) -> int:
        """
        S√©lectionne une action pour un √©tat continu
        
        Args:
            continuous_state: √âtat [position, vitesse]
            training: Si True, utilise epsilon-greedy
            
        Returns:
            Action s√©lectionn√©e
        """
        discrete_state = self.process_state(continuous_state)
        return super().select_action(discrete_state, training)
    
    def update_q_table(self, continuous_state: np.ndarray, action: int, 
                      reward: float, continuous_next_state: np.ndarray, done: bool):
        """
        Met √† jour la Q-table avec des √©tats continus
        
        Args:
            continuous_state: √âtat actuel [position, vitesse]
            action: Action prise
            reward: R√©compense re√ßue
            continuous_next_state: √âtat suivant [position, vitesse]
            done: √âpisode termin√©
        """
        state = self.process_state(continuous_state)
        next_state = self.process_state(continuous_next_state)
        super().update_q_table(state, action, reward, next_state, done)
    
    def train(self, env: gym.Env, episodes: int = 10000, 
             max_steps: int = 200, verbose: bool = True) -> List[float]:
        """
        Entra√Æne l'agent sur MountainCar
        
        Args:
            env: Environnement MountainCar
            episodes: Nombre d'√©pisodes d'entra√Ænement
            max_steps: Nombre maximum de pas par √©pisode
            verbose: Afficher les progr√®s
            
        Returns:
            Liste des r√©compenses par √©pisode
        """
        episode_rewards = []
        success_count = 0
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"[START] ENTRA√éNEMENT MOUNTAINCAR - {episodes} √©pisodes")
            print(f"{'='*80}\n")
        
        for episode in range(episodes):
            continuous_state, _ = env.reset()
            total_reward = 0
            steps = 0
            
            for step in range(max_steps):
                # S√©lection et ex√©cution de l'action
                action = self.select_action(continuous_state, training=True)
                next_continuous_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                # Mise √† jour de la Q-table
                self.update_q_table(continuous_state, action, reward, 
                                   next_continuous_state, done)
                
                continuous_state = next_continuous_state
                total_reward += reward
                steps += 1
                
                # V√©rification de succ√®s
                if continuous_state[0] >= 0.5:  # Position >= 0.5 = but atteint
                    success_count += 1
                
                if done:
                    break
            
            # Diminution d'epsilon
            self.decay_epsilon()
            
            episode_rewards.append(total_reward)
            
            # Affichage des progr√®s
            if verbose and (episode + 1) % 500 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                success_rate = (success_count / (episode + 1)) * 100
                print(f"√âpisode {episode + 1:5d}/{episodes} | "
                      f"R√©compense moy. (100 derniers): {avg_reward:7.2f} | "
                      f"Epsilon: {self.epsilon:.3f} | "
                      f"Succ√®s: {success_rate:.1f}%")
        
        self.training_rewards = episode_rewards
        
        if verbose:
            final_success_rate = (success_count / episodes) * 100
            print(f"\n{'='*80}")
            print(f"[OK] ENTRA√éNEMENT TERMIN√â")
            print(f"{'='*80}")
            print(f"R√©compense moyenne finale (100 derniers): {np.mean(episode_rewards[-100:]):.2f}")
            print(f"Taux de succ√®s: {final_success_rate:.1f}%")
            print(f"Epsilon final: {self.epsilon:.3f}")
            print(f"{'='*80}\n")
        
        return episode_rewards
    
    def evaluate(self, env: gym.Env, episodes: int = 100, 
                render: bool = False, verbose: bool = True) -> Tuple[List[float], dict]:
        """
        √âvalue l'agent sur MountainCar
        
        Args:
            env: Environnement MountainCar
            episodes: Nombre d'√©pisodes d'√©valuation
            render: Afficher les √©pisodes
            verbose: Afficher les r√©sultats
            
        Returns:
            Tuple (liste de r√©compenses, statistiques)
        """
        episode_rewards = []
        episode_lengths = []
        success_count = 0
        
        for episode in range(episodes):
            continuous_state, _ = env.reset()
            total_reward = 0
            steps = 0
            
            while steps < 200:
                action = self.select_action(continuous_state, training=False)
                continuous_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                total_reward += reward
                steps += 1
                
                if continuous_state[0] >= 0.5:
                    success_count += 1
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
            episode_lengths.append(steps)
        
        # Calcul des statistiques
        stats = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'max_reward': np.max(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'std_length': np.std(episode_lengths),
            'success_rate': (success_count / episodes) * 100,
            'total_episodes': episodes
        }
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"[PLOT] √âVALUATION MOUNTAINCAR - {episodes} √©pisodes")
            print(f"{'='*80}")
            print(f"R√©compense moyenne: {stats['mean_reward']:.2f} ¬± {stats['std_reward']:.2f}")
            print(f"R√©compense min/max: {stats['min_reward']:.2f} / {stats['max_reward']:.2f}")
            print(f"Longueur moyenne: {stats['mean_length']:.1f} ¬± {stats['std_length']:.1f} pas")
            print(f"Taux de succ√®s: {stats['success_rate']:.1f}%")
            print(f"{'='*80}\n")
        
        return episode_rewards, stats
    
    def get_action_name(self, action: int) -> str:
        """Retourne le nom d'une action"""
        action_names = {
            0: "‚Üê Gauche",
            1: "‚äô Rien",
            2: "‚Üí Droite"
        }
        return action_names.get(action, f"Action {action}")
    
    def get_state_analysis(self, continuous_state: np.ndarray) -> dict:
        """
        Analyse un √©tat et retourne des informations d√©taill√©es
        
        Args:
            continuous_state: √âtat [position, vitesse]
            
        Returns:
            Dictionnaire d'analyses
        """
        discrete_state = self.process_state(continuous_state)
        state_info = self.discretizer.get_state_info(continuous_state)
        
        # Valeurs Q pour cet √©tat
        q_values = self.q_table[discrete_state]
        best_action = np.argmax(q_values)
        
        return {
            **state_info,
            'discrete_state': discrete_state,
            'q_values': q_values,
            'best_action': best_action,
            'best_action_name': self.get_action_name(best_action),
            'q_value_best': q_values[best_action]
        }
    
    def visualize_policy(self, sample_states: Optional[List[Tuple[float, float]]] = None):
        """
        Visualise la politique apprise pour quelques √©tats
        
        Args:
            sample_states: Liste d'√©tats √† analyser (si None, utilise des √©tats par d√©faut)
        """
        if sample_states is None:
            # √âtats par d√©faut repr√©sentatifs
            sample_states = [
                (-1.2, 0.0),   # D√©part
                (-0.8, -0.05), # Vall√©e gauche avec vitesse gauche
                (-0.5, 0.0),   # Centre vall√©e
                (-0.2, 0.05),  # Mont√©e droite avec √©lan
                (0.0, 0.0),    # Milieu
                (0.3, 0.05),   # Proche du but
            ]
        
        print(f"\n{'='*80}")
        print("[MAP]  POLITIQUE APPRISE (√©chantillon d'√©tats)")
        print(f"{'='*80}\n")
        
        for position, velocity in sample_states:
            state = np.array([position, velocity])
            analysis = self.get_state_analysis(state)
            
            print(f"Position: {position:5.2f} | Vitesse: {velocity:6.3f}")
            print(f"  ‚Üí Action recommand√©e: {analysis['best_action_name']}")
            print(f"  ‚Üí Valeur Q: {analysis['q_value_best']:.2f}")
            print(f"  ‚Üí Progr√®s: {analysis['progress_percent']:.1f}%")
            print()
        
        print(f"{'='*80}\n")


def test_mountain_car_agent():
    """Test de l'agent MountainCar"""
    print("üß™ TEST DE L'AGENT MOUNTAINCAR\n")
    
    # Cr√©ation de l'environnement
    env = gym.make('MountainCar-v0')
    
    # Cr√©ation de l'agent
    agent = MountainCarAgent(
        n_position_bins=20,
        n_velocity_bins=20,
        learning_rate=0.1,
        discount_factor=0.99
    )
    
    # Test rapide d'entra√Ænement
    print("\nüèÉ Entra√Ænement rapide (1000 √©pisodes)...")
    agent.train(env, episodes=1000, verbose=True)
    
    # √âvaluation
    print("\n[PLOT] √âvaluation...")
    agent.evaluate(env, episodes=50, verbose=True)
    
    # Visualisation de la politique
    agent.visualize_policy()
    
    env.close()
    print("[OK] Test termin√©!")


if __name__ == "__main__":
    test_mountain_car_agent()
